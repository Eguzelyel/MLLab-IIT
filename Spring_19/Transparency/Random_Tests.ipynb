{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing former models with test_random data\n",
    "---------\n",
    "_How data is collected:_ Until now, we were using uncertainty sampling to label EDUs and use those EDUs as train and test data. This approach is problematic because test data doesn't represent the overall EDUs, a.k.a real world.\n",
    "\n",
    "Instead, I took 1000 random data points from UNLABELED EDUs, and labeled them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import labeled_functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold # Difference? (indices=None, or nothing)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.models import Model, save_model, load_model\n",
    "from keras.layers import Dense, LSTM, Input, Embedding, Flatten, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import text_to_word_sequence, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR, MNB\n",
    "labeledfunctions.load_labeled_neutrals() is modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled Data loaded.\n",
      "Test Data loaded.\n"
     ]
    }
   ],
   "source": [
    "X_corpus, y, test_corpus, test_y = labeled_functions.load_labeled_neutrals(path=r\"./../../Fall_18/edu/active_learning/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "709"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "def vectorize(ngram=(1,3), stop=[\"the\",\"a\",\"of\",\"and\",\"br\",\"to\"]):\n",
    "    return CountVectorizer(token_pattern=token, binary=True, ngram_range=ngram, stop_words=stop)\n",
    "\n",
    "\n",
    "vectorizer_one = vectorize(stop=[\"a\",\"of\",\"and\",\"br\",\"to\"])\n",
    "X_vector = vectorizer_one.fit_transform(X_corpus)\n",
    "test_vector = vectorizer_one.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3274, 13731, 14231, 14282, 14318, 27021, 27106, 33526, 33602,\n",
       "        33604, 36729, 38869, 39346, 62401, 65923, 65928]),)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vector.toarray()[3].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7207334273624824"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_vector,y)\n",
    "lr.score(test_vector,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[238,  13,   7],\n",
       "       [ 77,  31,  76],\n",
       "       [ 20,   5, 242]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = lr.predict(test_vector)\n",
    "confusion_matrix(test_y, pred, labels=[-1,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "0.7023977433004231\n",
      "0.7052186177715092\n",
      "(1, 2)\n",
      "0.7165021156558533\n",
      "0.7108603667136812\n",
      "(1, 3)\n",
      "0.7207334273624824\n",
      "0.7080394922425952\n",
      "(1, 4)\n",
      "0.7179125528913963\n",
      "0.7122708039492243\n"
     ]
    }
   ],
   "source": [
    "grams=[(1,1),(1,2),(1,3),(1,4)]\n",
    "\n",
    "for gram in grams:\n",
    "    print(gram)\n",
    "    vectorizer = vectorize(ngram=gram, stop=[\"a\",\"of\",\"and\",\"br\",\"to\"])\n",
    "    X_vector = vectorizer.fit_transform(X_corpus)\n",
    "    test_vector = vectorizer.transform(test_corpus)\n",
    "    \n",
    "    models = [LogisticRegression(), MultinomialNB()]\n",
    "    \n",
    "    for model in models:\n",
    "        model.fit(X_vector, y)\n",
    "        print(model.score(test_vector, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7258347978910369\n",
      "-1\n",
      "[ 4975  8422 15015 ... 65084 62115 27423]\n",
      "awful \t 2.34\n",
      "boring \t 2.24\n",
      "dull \t 2.23\n",
      "not bad \t -2.10\n",
      "beautiful \t -1.99\n",
      "annoying \t 1.77\n",
      "ridiculous \t 1.73\n",
      "fails \t 1.72\n",
      "waste \t 1.71\n",
      "great \t -1.71\n",
      "rare \t -1.67\n",
      "fantastic \t -1.67\n",
      "excellent \t -1.64\n",
      "worst \t 1.62\n",
      "poorly \t 1.61\n",
      "pointless \t 1.60\n",
      "predictable \t 1.55\n",
      "loved \t -1.55\n",
      "terrible \t 1.54\n",
      "unfunny \t 1.50\n",
      "\n",
      "0\n",
      "[19904  8422  7345 ... 47693   924 53926]\n",
      "friend \t 1.82\n",
      "boring \t -1.63\n",
      "best friend \t 1.41\n",
      "dull \t -1.31\n",
      "beloved \t 1.30\n",
      "unsurprisingly \t 1.29\n",
      "surprisingly \t 1.28\n",
      "him \t 1.22\n",
      "funeral \t 1.18\n",
      "guy \t 1.18\n",
      "rarely \t 1.17\n",
      "awful \t -1.17\n",
      "performance \t -1.06\n",
      "noir \t 1.02\n",
      "it is not \t 1.01\n",
      "waste \t -1.00\n",
      "my \t -0.99\n",
      "7/10 \t -0.97\n",
      "were \t -0.97\n",
      "acting \t -0.97\n",
      "\n",
      "1\n",
      "[38912 43252 16733 ... 21820 63409 34075]\n",
      "not bad \t 2.64\n",
      "poor \t -2.40\n",
      "excellent \t 2.03\n",
      "7/10 \t 2.03\n",
      "amazing \t 1.90\n",
      "fascinating \t 1.84\n",
      "beautiful \t 1.82\n",
      "8/10 \t 1.81\n",
      "fantastic \t 1.79\n",
      "great \t 1.78\n",
      "perfectly \t 1.70\n",
      "10/10 \t 1.65\n",
      "unfortunately \t -1.63\n",
      "awful \t -1.60\n",
      "fun \t 1.60\n",
      "gem \t 1.58\n",
      "enjoyable \t 1.58\n",
      "best friend \t -1.56\n",
      "incredible \t 1.56\n",
      "bad \t -1.54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer=vectorize(ngram=(1,3), stop=[\"a\",\"of\",\"and\",\"br\",\"to\"])\n",
    "X_vector = vectorizer.fit_transform(X_corpus)\n",
    "test_vector = vectorizer.transform(test_corpus)\n",
    "\n",
    "\n",
    "lr.fit(X_vector,y)\n",
    "print(lr.score(test_vector,test_y))\n",
    "\n",
    "# Find biggest coefficients.\n",
    "\n",
    "for i in range(3):\n",
    "    print(lr.classes_[i])\n",
    "    \n",
    "\n",
    "    inds = np.argsort(np.abs(lr.coef_[i]))[::-1]\n",
    "\n",
    "    print(inds)\n",
    "    \n",
    "    for j in inds[:20]:\n",
    "        print(\"%s \\t %0.2f\" %(vectorizer.get_feature_names()[j], lr.coef_[i][j]))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFNN\n",
    "With the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0, -1,  0, -1,  1]), array([[1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.]], dtype=float32), array([[0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]], dtype=float32))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat, test_y_cat = to_categorical(y,num_classes=3), to_categorical(test_y,num_classes=3)\n",
    "y[:5], y_cat[:5], test_y_cat[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[55, 56, 144, 116, 136, 31, 52, 75, 79, 101, 39], [132, 72, 57, 20, 4]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine train and test to one-hot-encode. Then split back.\n",
    "test_len = len(test_corpus)\n",
    "X_full = np.concatenate((X_corpus, test_corpus), axis=0)\n",
    "y_full = np.concatenate((y, test_y), axis=0)\n",
    "\n",
    "# Maximum sequence length\n",
    "X_sequence = []\n",
    "for i in X_full:\n",
    "    X_sequence.append(text_to_word_sequence(i))\n",
    "max_length = len(max(X_sequence,key=len))\n",
    "\n",
    "X_corpus_one_hot = []\n",
    "for i in X_full:\n",
    "    X_corpus_one_hot.append(one_hot(i, round(max_length*1.1)))\n",
    "\n",
    "X_corpus_one_hot[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5232, 709)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_seq = pad_sequences(X_corpus_one_hot)\n",
    "X_one_hot, test_one_hot = padded_seq[:-test_len], padded_seq[-test_len:]\n",
    "len(X_one_hot), len(test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "input_nodes= Input(shape=(X_one_hot.shape[1],))\n",
    "e = Embedding(round(max_length*1.1),\n",
    "              100,\n",
    "              input_length=X_one_hot.shape[1],\n",
    "              trainable=True)(input_nodes)\n",
    "flat= Flatten()(e)\n",
    "dense1 = Dense(100, activation='tanh', kernel_regularizer=regularizers.l2(0.1))(flat)\n",
    "# drop = Dropout(0.2)(dense1)\n",
    "dense2 = Dense(10, activation='tanh')(dense1)\n",
    "# drop2 = Dropout(0.2)(dense2)\n",
    "\n",
    "# dense2 = Dense(30, activation='sigmoid')(dense1)\n",
    "\n",
    "output_nodes=Dense(3, activation='softmax')(dense2)\n",
    "# output_nodes=Dense(1, activation='sigmoid')(dense1)\n",
    "\n",
    "#Build model\n",
    "model = Model(inputs=input_nodes, outputs=output_nodes)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5232/5232 [==============================] - 4s 717us/step - loss: 2.8333 - acc: 0.3502 1s - loss: 3.8675 - acc:  - ETA: 0s - loss: 3.3450 - a\n",
      "Epoch 2/50\n",
      "5232/5232 [==============================] - 3s 536us/step - loss: 1.0982 - acc: 0.4014\n",
      "Epoch 3/50\n",
      "5232/5232 [==============================] - 3s 537us/step - loss: 1.0886 - acc: 0.4807\n",
      "Epoch 4/50\n",
      "5232/5232 [==============================] - 3s 585us/step - loss: 1.0948 - acc: 0.5019\n",
      "Epoch 5/50\n",
      "5232/5232 [==============================] - 3s 639us/step - loss: 1.0804 - acc: 0.5151\n",
      "Epoch 6/50\n",
      "5232/5232 [==============================] - 3s 551us/step - loss: 1.0772 - acc: 0.5315\n",
      "Epoch 7/50\n",
      "5232/5232 [==============================] - 4s 689us/step - loss: 1.0610 - acc: 0.5428\n",
      "Epoch 8/50\n",
      "5232/5232 [==============================] - 3s 658us/step - loss: 1.0560 - acc: 0.5558\n",
      "Epoch 9/50\n",
      "5232/5232 [==============================] - 4s 692us/step - loss: 1.0418 - acc: 0.5640\n",
      "Epoch 10/50\n",
      "5232/5232 [==============================] - 4s 793us/step - loss: 1.0380 - acc: 0.5648\n",
      "Epoch 11/50\n",
      "5232/5232 [==============================] - 4s 696us/step - loss: 1.0287 - acc: 0.5700\n",
      "Epoch 12/50\n",
      "5232/5232 [==============================] - 3s 622us/step - loss: 1.0152 - acc: 0.5917\n",
      "Epoch 13/50\n",
      "5232/5232 [==============================] - 4s 729us/step - loss: 1.0036 - acc: 0.5986\n",
      "Epoch 14/50\n",
      "5232/5232 [==============================] - 3s 638us/step - loss: 0.9945 - acc: 0.5969\n",
      "Epoch 15/50\n",
      "5232/5232 [==============================] - 3s 534us/step - loss: 0.9943 - acc: 0.6005\n",
      "Epoch 16/50\n",
      "5232/5232 [==============================] - 3s 638us/step - loss: 0.9793 - acc: 0.6143\n",
      "Epoch 17/50\n",
      "5232/5232 [==============================] - 3s 599us/step - loss: 0.9657 - acc: 0.6235\n",
      "Epoch 18/50\n",
      "5232/5232 [==============================] - 3s 540us/step - loss: 0.9640 - acc: 0.6206\n",
      "Epoch 19/50\n",
      "5232/5232 [==============================] - 3s 608us/step - loss: 0.9541 - acc: 0.6277\n",
      "Epoch 20/50\n",
      "5232/5232 [==============================] - 3s 595us/step - loss: 0.9343 - acc: 0.6420\n",
      "Epoch 21/50\n",
      "5232/5232 [==============================] - 3s 544us/step - loss: 0.9357 - acc: 0.6403\n",
      "Epoch 22/50\n",
      "5232/5232 [==============================] - 3s 573us/step - loss: 0.9318 - acc: 0.6422\n",
      "Epoch 23/50\n",
      "5232/5232 [==============================] - 3s 601us/step - loss: 0.9182 - acc: 0.6456\n",
      "Epoch 24/50\n",
      "5232/5232 [==============================] - 3s 610us/step - loss: 0.9130 - acc: 0.6510\n",
      "Epoch 25/50\n",
      "5232/5232 [==============================] - 3s 528us/step - loss: 0.9252 - acc: 0.6567\n",
      "Epoch 26/50\n",
      "5232/5232 [==============================] - 3s 569us/step - loss: 0.9061 - acc: 0.6525\n",
      "Epoch 27/50\n",
      "5232/5232 [==============================] - 3s 599us/step - loss: 0.9197 - acc: 0.6535\n",
      "Epoch 28/50\n",
      "5232/5232 [==============================] - 3s 529us/step - loss: 0.8984 - acc: 0.6550\n",
      "Epoch 29/50\n",
      "5232/5232 [==============================] - 3s 600us/step - loss: 0.8681 - acc: 0.6751\n",
      "Epoch 30/50\n",
      "5232/5232 [==============================] - 3s 609us/step - loss: 0.8687 - acc: 0.6720\n",
      "Epoch 31/50\n",
      "5232/5232 [==============================] - 3s 638us/step - loss: 0.8597 - acc: 0.6753\n",
      "Epoch 32/50\n",
      "5232/5232 [==============================] - 3s 657us/step - loss: 0.8685 - acc: 0.6768\n",
      "Epoch 33/50\n",
      "5232/5232 [==============================] - 3s 603us/step - loss: 0.8742 - acc: 0.6812\n",
      "Epoch 34/50\n",
      "5232/5232 [==============================] - 3s 663us/step - loss: 0.8552 - acc: 0.6864\n",
      "Epoch 35/50\n",
      "5232/5232 [==============================] - 3s 662us/step - loss: 0.8553 - acc: 0.6818\n",
      "Epoch 36/50\n",
      "5232/5232 [==============================] - 4s 796us/step - loss: 0.8487 - acc: 0.6821\n",
      "Epoch 37/50\n",
      "5232/5232 [==============================] - 4s 822us/step - loss: 0.8429 - acc: 0.6969\n",
      "Epoch 38/50\n",
      "5232/5232 [==============================] - 3s 639us/step - loss: 0.8335 - acc: 0.6955\n",
      "Epoch 39/50\n",
      "5232/5232 [==============================] - 4s 669us/step - loss: 0.8196 - acc: 0.6913\n",
      "Epoch 40/50\n",
      "5232/5232 [==============================] - 4s 695us/step - loss: 0.8333 - acc: 0.6971\n",
      "Epoch 41/50\n",
      "5232/5232 [==============================] - 3s 617us/step - loss: 0.8325 - acc: 0.6923\n",
      "Epoch 42/50\n",
      "5232/5232 [==============================] - 3s 636us/step - loss: 0.8034 - acc: 0.7089\n",
      "Epoch 43/50\n",
      "5232/5232 [==============================] - 4s 719us/step - loss: 0.8160 - acc: 0.7034\n",
      "Epoch 44/50\n",
      "5232/5232 [==============================] - 3s 606us/step - loss: 0.8074 - acc: 0.7072\n",
      "Epoch 45/50\n",
      "5232/5232 [==============================] - 3s 580us/step - loss: 0.8021 - acc: 0.7141\n",
      "Epoch 46/50\n",
      "5232/5232 [==============================] - 3s 524us/step - loss: 0.7972 - acc: 0.7087\n",
      "Epoch 47/50\n",
      "5232/5232 [==============================] - 3s 548us/step - loss: 0.8125 - acc: 0.7066\n",
      "Epoch 48/50\n",
      "5232/5232 [==============================] - 3s 557us/step - loss: 0.8043 - acc: 0.7154\n",
      "Epoch 49/50\n",
      "5232/5232 [==============================] - 3s 538us/step - loss: 0.7909 - acc: 0.7131\n",
      "Epoch 50/50\n",
      "5232/5232 [==============================] - 3s 594us/step - loss: 0.7906 - acc: 0.7177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13106aeb8>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_one_hot, y_cat, batch_size=64, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "709/709 [==============================] - 0s 388us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.384461901251789, 0.51198871654415]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_one_hot, test_y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569/569 [==============================] - 0s 503us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3962477732626541, 0.536028118669882]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_one_hot, test_y_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1.3962477732626541, 0.536028118669882]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pred = model.predict(test_one_hot)\n",
    "model_pred_trinary=[]\n",
    "for i in model_pred:\n",
    "#     if max(i)\n",
    "    lab = np.where(i ==max(i))[0][0]\n",
    "\n",
    "    if lab==0:\n",
    "        model_pred_trinary.append(-1)\n",
    "    elif lab==1:\n",
    "        model_pred_trinary.append(1)\n",
    "    else:\n",
    "        model_pred_trinary.append(0)\n",
    "model_pred_trinary[100:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 41,  47, 118],\n",
       "       [ 32, 129,  51],\n",
       "       [ 58,  56,  37]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y, model_pred_trinary, labels=[-1,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Networks\n",
    "\n",
    "https://arxiv.org/pdf/1410.3916.pdf\n",
    "\n",
    "I watched the movie. It was awful.\n",
    "\n",
    "----\n",
    "\n",
    "Jack is in the kitchen.\n",
    "\n",
    "Jack went to bathroom.\n",
    "\n",
    "Where is Jack? -> bathroom\n",
    "\n",
    "Jack came back. \n",
    "\n",
    "Where is Jack? -> kitchen\n",
    "\n",
    "------\n",
    "\n",
    "Joe went to the kitchen.\n",
    "\n",
    "Fred went to the kitchen. \n",
    "\n",
    "Joe picked up the milk. \n",
    "\n",
    "Joe travelled to the office. \n",
    "\n",
    "Joe left the milk. \n",
    "\n",
    "Joe went to the bathroom. \n",
    "\n",
    "Where is the milk now? A: office\n",
    "\n",
    "Where is Joe? A: bathroom\n",
    "\n",
    "Where was Joe before the office? A: kitchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
