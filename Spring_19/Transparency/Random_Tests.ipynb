{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing former models with test_random data\n",
    "---------\n",
    "_How data is collected:_ Until now, we were using uncertainty sampling to label EDUs and use those EDUs as train and test data. This approach is problematic because test data doesn't represent the overall EDUs, a.k.a real world.\n",
    "\n",
    "Instead, I took 1000 random data points from UNLABELED EDUs, and labeled them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import labeled_functions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold # Difference? (indices=None, or nothing)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.models import Model, save_model, load_model\n",
    "from keras.layers import Dense, LSTM, Input, Embedding, Flatten, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import text_to_word_sequence, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR, MNB\n",
    "labeledfunctions.load_labeled_neutrals() is modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled Data loaded.\n",
      "Test Data loaded.\n"
     ]
    }
   ],
   "source": [
    "X_corpus, y, test_corpus, test_y = labeled_functions.load_labeled_neutrals(path=r\"./../../Fall_18/edu/active_learning/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "709"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "def vectorize(ngram=(1,3), stop=[\"the\",\"a\",\"of\",\"and\",\"br\",\"to\"]):\n",
    "    return CountVectorizer(token_pattern=token, binary=True, ngram_range=ngram, stop_words=stop)\n",
    "\n",
    "\n",
    "vectorizer_one = vectorize(stop=[\"a\",\"of\",\"and\",\"br\",\"to\"])\n",
    "X_vector = vectorizer_one.fit_transform(X_corpus)\n",
    "test_vector = vectorizer_one.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3274, 13731, 14231, 14282, 14318, 27021, 27106, 33526, 33602,\n",
       "        33604, 36729, 38869, 39346, 62401, 65923, 65928]),)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vector.toarray()[3].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7207334273624824"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_vector,y)\n",
    "lr.score(test_vector,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[238,  13,   7],\n",
       "       [ 77,  31,  76],\n",
       "       [ 20,   5, 242]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = lr.predict(test_vector)\n",
    "confusion_matrix(test_y, pred, labels=[-1,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "0.7023977433004231\n",
      "0.7052186177715092\n",
      "(1, 2)\n",
      "0.7165021156558533\n",
      "0.7108603667136812\n",
      "(1, 3)\n",
      "0.7207334273624824\n",
      "0.7080394922425952\n",
      "(1, 4)\n",
      "0.7179125528913963\n",
      "0.7122708039492243\n"
     ]
    }
   ],
   "source": [
    "grams=[(1,1),(1,2),(1,3),(1,4)]\n",
    "\n",
    "for gram in grams:\n",
    "    print(gram)\n",
    "    vectorizer = vectorize(ngram=gram, stop=[\"a\",\"of\",\"and\",\"br\",\"to\"])\n",
    "    X_vector = vectorizer.fit_transform(X_corpus)\n",
    "    test_vector = vectorizer.transform(test_corpus)\n",
    "    \n",
    "    models = [LogisticRegression(), MultinomialNB()]\n",
    "    \n",
    "    for model in models:\n",
    "        model.fit(X_vector, y)\n",
    "        print(model.score(test_vector, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7258347978910369\n",
      "-1\n",
      "[ 4975  8422 15015 ... 65084 62115 27423]\n",
      "awful \t 2.34\n",
      "boring \t 2.24\n",
      "dull \t 2.23\n",
      "not bad \t -2.10\n",
      "beautiful \t -1.99\n",
      "annoying \t 1.77\n",
      "ridiculous \t 1.73\n",
      "fails \t 1.72\n",
      "waste \t 1.71\n",
      "great \t -1.71\n",
      "rare \t -1.67\n",
      "fantastic \t -1.67\n",
      "excellent \t -1.64\n",
      "worst \t 1.62\n",
      "poorly \t 1.61\n",
      "pointless \t 1.60\n",
      "predictable \t 1.55\n",
      "loved \t -1.55\n",
      "terrible \t 1.54\n",
      "unfunny \t 1.50\n",
      "\n",
      "0\n",
      "[19904  8422  7345 ... 47693   924 53926]\n",
      "friend \t 1.82\n",
      "boring \t -1.63\n",
      "best friend \t 1.41\n",
      "dull \t -1.31\n",
      "beloved \t 1.30\n",
      "unsurprisingly \t 1.29\n",
      "surprisingly \t 1.28\n",
      "him \t 1.22\n",
      "funeral \t 1.18\n",
      "guy \t 1.18\n",
      "rarely \t 1.17\n",
      "awful \t -1.17\n",
      "performance \t -1.06\n",
      "noir \t 1.02\n",
      "it is not \t 1.01\n",
      "waste \t -1.00\n",
      "my \t -0.99\n",
      "7/10 \t -0.97\n",
      "were \t -0.97\n",
      "acting \t -0.97\n",
      "\n",
      "1\n",
      "[38912 43252 16733 ... 21820 63409 34075]\n",
      "not bad \t 2.64\n",
      "poor \t -2.40\n",
      "excellent \t 2.03\n",
      "7/10 \t 2.03\n",
      "amazing \t 1.90\n",
      "fascinating \t 1.84\n",
      "beautiful \t 1.82\n",
      "8/10 \t 1.81\n",
      "fantastic \t 1.79\n",
      "great \t 1.78\n",
      "perfectly \t 1.70\n",
      "10/10 \t 1.65\n",
      "unfortunately \t -1.63\n",
      "awful \t -1.60\n",
      "fun \t 1.60\n",
      "gem \t 1.58\n",
      "enjoyable \t 1.58\n",
      "best friend \t -1.56\n",
      "incredible \t 1.56\n",
      "bad \t -1.54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer=vectorize(ngram=(1,3), stop=[\"a\",\"of\",\"and\",\"br\",\"to\"])\n",
    "X_vector = vectorizer.fit_transform(X_corpus)\n",
    "test_vector = vectorizer.transform(test_corpus)\n",
    "\n",
    "\n",
    "lr.fit(X_vector,y)\n",
    "print(lr.score(test_vector,test_y))\n",
    "\n",
    "# Find biggest coefficients.\n",
    "\n",
    "for i in range(3):\n",
    "    print(lr.classes_[i])\n",
    "    \n",
    "\n",
    "    inds = np.argsort(np.abs(lr.coef_[i]))[::-1]\n",
    "\n",
    "    print(inds)\n",
    "    \n",
    "    for j in inds[:20]:\n",
    "        print(\"%s \\t %0.2f\" %(vectorizer.get_feature_names()[j], lr.coef_[i][j]))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFNN\n",
    "With the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0, -1,  0, -1,  1]), array([[1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.]], dtype=float32), array([[0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]], dtype=float32))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cat, test_y_cat = to_categorical(y,num_classes=3), to_categorical(test_y,num_classes=3)\n",
    "y[:5], y_cat[:5], test_y_cat[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[55, 56, 144, 116, 136, 31, 52, 75, 79, 101, 39], [132, 72, 57, 20, 4]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine train and test to one-hot-encode. Then split back.\n",
    "test_len = len(test_corpus)\n",
    "X_full = np.concatenate((X_corpus, test_corpus), axis=0)\n",
    "y_full = np.concatenate((y, test_y), axis=0)\n",
    "\n",
    "# Maximum sequence length\n",
    "X_sequence = []\n",
    "for i in X_full:\n",
    "    X_sequence.append(text_to_word_sequence(i))\n",
    "max_length = len(max(X_sequence,key=len))\n",
    "\n",
    "X_corpus_one_hot = []\n",
    "for i in X_full:\n",
    "    X_corpus_one_hot.append(one_hot(i, round(max_length*1.1)))\n",
    "\n",
    "X_corpus_one_hot[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5232, 709)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_seq = pad_sequences(X_corpus_one_hot)\n",
    "X_one_hot, test_one_hot = padded_seq[:-test_len], padded_seq[-test_len:]\n",
    "len(X_one_hot), len(test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "input_nodes= Input(shape=(X_one_hot.shape[1],))\n",
    "e = Embedding(round(max_length*1.1),\n",
    "              100,\n",
    "              input_length=X_one_hot.shape[1],\n",
    "              trainable=True)(input_nodes)\n",
    "flat= Flatten()(e)\n",
    "dense1 = Dense(100, activation='tanh', kernel_regularizer=regularizers.l2(0.1))(flat)\n",
    "# drop = Dropout(0.2)(dense1)\n",
    "dense2 = Dense(10, activation='tanh')(dense1)\n",
    "# drop2 = Dropout(0.2)(dense2)\n",
    "\n",
    "# dense2 = Dense(30, activation='sigmoid')(dense1)\n",
    "\n",
    "output_nodes=Dense(3, activation='softmax')(dense2)\n",
    "# output_nodes=Dense(1, activation='sigmoid')(dense1)\n",
    "\n",
    "#Build model\n",
    "model = Model(inputs=input_nodes, outputs=output_nodes)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5232/5232 [==============================] - 3s 637us/step - loss: 2.7783 - acc: 0.3563\n",
      "Epoch 2/50\n",
      "5232/5232 [==============================] - 3s 645us/step - loss: 1.0988 - acc: 0.3987\n",
      "Epoch 3/50\n",
      "5232/5232 [==============================] - 3s 614us/step - loss: 1.0907 - acc: 0.4413\n",
      "Epoch 4/50\n",
      "5232/5232 [==============================] - 3s 592us/step - loss: 1.0872 - acc: 0.4709\n",
      "Epoch 5/50\n",
      "5232/5232 [==============================] - 3s 614us/step - loss: 1.0853 - acc: 0.4847\n",
      "Epoch 6/50\n",
      "5232/5232 [==============================] - 3s 647us/step - loss: 1.0891 - acc: 0.5086\n",
      "Epoch 7/50\n",
      "5232/5232 [==============================] - 3s 573us/step - loss: 1.0823 - acc: 0.5256\n",
      "Epoch 8/50\n",
      "5232/5232 [==============================] - 3s 562us/step - loss: 1.0622 - acc: 0.5415\n",
      "Epoch 9/50\n",
      "5232/5232 [==============================] - 3s 597us/step - loss: 1.0574 - acc: 0.5539\n",
      "Epoch 10/50\n",
      "5232/5232 [==============================] - 3s 591us/step - loss: 1.0505 - acc: 0.5625\n",
      "Epoch 11/50\n",
      "5232/5232 [==============================] - 3s 530us/step - loss: 1.0354 - acc: 0.5774\n",
      "Epoch 12/50\n",
      "5232/5232 [==============================] - 3s 556us/step - loss: 1.0244 - acc: 0.5784\n",
      "Epoch 13/50\n",
      "5232/5232 [==============================] - 3s 612us/step - loss: 1.0182 - acc: 0.5891\n",
      "Epoch 14/50\n",
      "5232/5232 [==============================] - 3s 548us/step - loss: 1.0097 - acc: 0.5898\n",
      "Epoch 15/50\n",
      "5232/5232 [==============================] - 3s 581us/step - loss: 0.9933 - acc: 0.6070\n",
      "Epoch 16/50\n",
      "5232/5232 [==============================] - 3s 572us/step - loss: 0.9960 - acc: 0.6011\n",
      "Epoch 17/50\n",
      "5232/5232 [==============================] - 3s 623us/step - loss: 0.9843 - acc: 0.6095\n",
      "Epoch 18/50\n",
      "5232/5232 [==============================] - 3s 528us/step - loss: 0.9755 - acc: 0.6174\n",
      "Epoch 19/50\n",
      "5232/5232 [==============================] - 3s 556us/step - loss: 0.9663 - acc: 0.6216\n",
      "Epoch 20/50\n",
      "5232/5232 [==============================] - 3s 540us/step - loss: 0.9478 - acc: 0.6346\n",
      "Epoch 21/50\n",
      "5232/5232 [==============================] - 3s 525us/step - loss: 0.9431 - acc: 0.6395\n",
      "Epoch 22/50\n",
      "5232/5232 [==============================] - 3s 542us/step - loss: 0.9488 - acc: 0.6416 0s - loss: 0.9487 -\n",
      "Epoch 23/50\n",
      "5232/5232 [==============================] - 3s 613us/step - loss: 0.9338 - acc: 0.6390\n",
      "Epoch 24/50\n",
      "5232/5232 [==============================] - 3s 592us/step - loss: 0.9270 - acc: 0.6441\n",
      "Epoch 25/50\n",
      "5232/5232 [==============================] - 3s 551us/step - loss: 0.9017 - acc: 0.6521\n",
      "Epoch 26/50\n",
      "5232/5232 [==============================] - 3s 591us/step - loss: 0.9120 - acc: 0.6477\n",
      "Epoch 27/50\n",
      "5232/5232 [==============================] - 3s 568us/step - loss: 0.9040 - acc: 0.6581\n",
      "Epoch 28/50\n",
      "5232/5232 [==============================] - 3s 538us/step - loss: 0.8869 - acc: 0.6606\n",
      "Epoch 29/50\n",
      "5232/5232 [==============================] - 3s 574us/step - loss: 0.8918 - acc: 0.6665\n",
      "Epoch 30/50\n",
      "5232/5232 [==============================] - 3s 599us/step - loss: 0.8779 - acc: 0.6776\n",
      "Epoch 31/50\n",
      "5232/5232 [==============================] - 3s 576us/step - loss: 0.8723 - acc: 0.6760\n",
      "Epoch 32/50\n",
      "5232/5232 [==============================] - 3s 539us/step - loss: 0.8668 - acc: 0.6783\n",
      "Epoch 33/50\n",
      "5232/5232 [==============================] - 3s 572us/step - loss: 0.8530 - acc: 0.6850\n",
      "Epoch 34/50\n",
      "5232/5232 [==============================] - 3s 616us/step - loss: 0.8580 - acc: 0.6821\n",
      "Epoch 35/50\n",
      "5232/5232 [==============================] - 3s 526us/step - loss: 0.8722 - acc: 0.6865\n",
      "Epoch 36/50\n",
      "5232/5232 [==============================] - 3s 551us/step - loss: 0.8373 - acc: 0.6900\n",
      "Epoch 37/50\n",
      "5232/5232 [==============================] - 4s 721us/step - loss: 0.8339 - acc: 0.6950\n",
      "Epoch 38/50\n",
      "5232/5232 [==============================] - 3s 623us/step - loss: 0.8351 - acc: 0.6909\n",
      "Epoch 39/50\n",
      "5232/5232 [==============================] - 3s 586us/step - loss: 0.8173 - acc: 0.7024\n",
      "Epoch 40/50\n",
      "5232/5232 [==============================] - 3s 559us/step - loss: 0.8299 - acc: 0.7026\n",
      "Epoch 41/50\n",
      "5232/5232 [==============================] - 3s 515us/step - loss: 0.8367 - acc: 0.6982\n",
      "Epoch 42/50\n",
      "5232/5232 [==============================] - 3s 540us/step - loss: 0.8101 - acc: 0.7062\n",
      "Epoch 43/50\n",
      "5232/5232 [==============================] - 4s 677us/step - loss: 0.8047 - acc: 0.7070\n",
      "Epoch 44/50\n",
      "5232/5232 [==============================] - 3s 622us/step - loss: 0.8117 - acc: 0.7116\n",
      "Epoch 45/50\n",
      "5232/5232 [==============================] - 4s 717us/step - loss: 0.8105 - acc: 0.7051\n",
      "Epoch 46/50\n",
      "5232/5232 [==============================] - 4s 683us/step - loss: 0.8015 - acc: 0.7175\n",
      "Epoch 47/50\n",
      "5232/5232 [==============================] - 3s 656us/step - loss: 0.8031 - acc: 0.7127\n",
      "Epoch 48/50\n",
      "5232/5232 [==============================] - 4s 689us/step - loss: 0.7859 - acc: 0.7171\n",
      "Epoch 49/50\n",
      "5232/5232 [==============================] - 3s 564us/step - loss: 0.7913 - acc: 0.7215\n",
      "Epoch 50/50\n",
      "5232/5232 [==============================] - 3s 632us/step - loss: 0.7873 - acc: 0.7146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12ad2d9b0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_one_hot, y_cat, batch_size=64, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569/569 [==============================] - 0s 503us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3962477732626541, 0.536028118669882]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_one_hot, test_y_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1.3962477732626541, 0.536028118669882]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pred = model.predict(test_one_hot)\n",
    "model_pred_trinary=[]\n",
    "for i in model_pred:\n",
    "#     if max(i)\n",
    "    lab = np.where(i ==max(i))[0][0]\n",
    "\n",
    "    if lab==0:\n",
    "        model_pred_trinary.append(-1)\n",
    "    elif lab==1:\n",
    "        model_pred_trinary.append(1)\n",
    "    else:\n",
    "        model_pred_trinary.append(0)\n",
    "model_pred_trinary[100:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 41,  47, 118],\n",
       "       [ 32, 129,  51],\n",
       "       [ 58,  56,  37]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y, model_pred_trinary, labels=[-1,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Networks\n",
    "\n",
    "https://arxiv.org/pdf/1410.3916.pdf\n",
    "\n",
    "I watched the movie. It was awful.\n",
    "\n",
    "----\n",
    "\n",
    "Jack is in the kitchen.\n",
    "\n",
    "Jack went to bathroom.\n",
    "\n",
    "Where is Jack? -> bathroom\n",
    "\n",
    "Jack came back. \n",
    "\n",
    "Where is Jack? -> kitchen\n",
    "\n",
    "------\n",
    "\n",
    "Joe went to the kitchen.\n",
    "\n",
    "Fred went to the kitchen. \n",
    "\n",
    "Joe picked up the milk. \n",
    "\n",
    "Joe travelled to the office. \n",
    "\n",
    "Joe left the milk. \n",
    "\n",
    "Joe went to the bathroom. \n",
    "\n",
    "Where is the milk now? A: office\n",
    "\n",
    "Where is Joe? A: bathroom\n",
    "\n",
    "Where was Joe before the office? A: kitchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
