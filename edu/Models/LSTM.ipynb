{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__QUESTIONS__\n",
    "\n",
    "- Could I use CountVectorizer?\n",
    "- Why model.score() gives two different results?\n",
    "- Is there an easier way to classify in binary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import labeled_functions\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import LSTM, Dense, Input, Embedding, LeakyReLU\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence, one_hot, Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, precision_score, precision_recall_fscore_support, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled Data loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['interspersed', 'with', 'multiple', 'boring', 'sex', 'scene'], 0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_corpus, y = labeled_functions.load_with_path(path=r\"/Users/ekremguzelyel/Desktop/Assignments/Research/MLLab-IIT/edu/active_learning\")\n",
    "X_sequence = []\n",
    "for i in X_corpus:\n",
    "#     print(type(i))\n",
    "    X_sequence.append(text_to_word_sequence(i))\n",
    "X_train_sequence, X_test_sequence, y_train, y_test = train_test_split(X_sequence, y, test_size=1./3, random_state=42)\n",
    "# X_train_vector , y_train, X_test_vector , y_test = labeled_functions.split_and_vectorize()\n",
    "# for i in X_train_vector:\n",
    "#     X_sequence.text_to_word_sequence(i)\n",
    "# X_train_vector.shape, X_test_vector.shape, len(y_train)\n",
    "X_train_sequence[0], y_train[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import glove\n",
    "c = X_train_sequence.copy()\n",
    "for i in c:\n",
    "    i = [glove[x] for x in i]\n",
    "#     y = [2*a for a in x if a % 2 == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum sequence length\n",
    "max_length=0\n",
    "for i in X_sequence:\n",
    "    max_length = [len(set(i)) if len(set(i)) > max_length else max_length][0]\n",
    "len(X_train_sequence)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[54, 49, 84, 40, 1, 41, 67, 83, 84, 57], [16, 58], [24, 86, 17, 72]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_corpus_one_hot = []\n",
    "for i in X_corpus:\n",
    "    X_corpus_one_hot.append(one_hot(i, round(max_length*1.2)))\n",
    "\n",
    "X_corpus_one_hot[4:7]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Here is why this code doesn't work:\n",
    "# We are trying to first convert the corpus into sequence, which is a list,\n",
    "#    then, try to one_hot encode. What we should do is use corpus and one_hot encode \n",
    "#    directly from text. Which is the above row.\n",
    "X_corpus_one_hot = []\n",
    "for i in X_sequence:\n",
    "    print(i)\n",
    "    X_corpus_one_hot.append(one_hot(i, round(max_length*1.2)))\n",
    "    \n",
    "X_corpus_one_hot[4:15]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_sequence"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len_ = np.zeros(len(X_train_sequence))\n",
    "\n",
    "for i in range(len(X_train_sequence)):\n",
    "    len_[i] = len(X_train_sequence[i])\n",
    "len_ = np.asarray(len_)\n",
    "print(np.max(len_))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len_ = np.zeros(len(X_corpus))\n",
    "\n",
    "for i in range(len(X_corpus)):\n",
    "    len_[i] = len(X_corpus[i])\n",
    "len_ = np.asarray(len_)\n",
    "print(np.max(len_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5, 83], 'compellingly awful ')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_corpus_one_hot[16], X_corpus[16] # the occurs twice (number 83) -> Checkmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_seq_corpus = pad_sequences(X_corpus_one_hot)\n",
    "X_train_padded_seq, X_test_padded_seq, y_train, y_test = train_test_split(padded_seq_corpus, y,\n",
    "                                                                          test_size=1./3, random_state=42) \n",
    "# padded_seq_corpus.shape\n",
    "len(padded_seq_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 35, 33,\n",
       "        34, 27,  3, 17], dtype=int32),\n",
       " 0,\n",
       " array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24, 91, 62,\n",
       "        72,  8, 22, 67], dtype=int32),\n",
       " 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded_seq[0], y_train[0], X_test_padded_seq[0], y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Network\n",
    "# from keras.utils import plot_model\n",
    "input_layer = Input(shape=(X_train_padded_seq.shape[1],))\n",
    "\n",
    "# CHANGE EMBEDDING TO GLOVE or WORD2VEC\n",
    "# embed_layer = Embedding(output_dim=64, input_dim=X_train_padded_seq.shape[1],\n",
    "#                         input_length=X_train_padded_seq.shape[1])(input_layer)\n",
    "e = Embedding(vocab_size,\n",
    "              100,\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=X_train_padded_seq.shape[1],\n",
    "              trainable=False)(input_layer)\n",
    "hidden1 = LSTM(32, return_sequences=False)(e)\n",
    "hidden2 = Dense(10, activation='relu')(hidden1)\n",
    "output_layer = Dense(1, activation='sigmoid')(hidden2)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "# summarize layers\n",
    "print(model.summary())\n",
    "# plot graph\n",
    "# plot_model(model, to_file='recurrent_neural_network.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train_padded_seq, y_train, epochs=20, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hist_test = model.fit(X_test_padded_seq, y_test, epochs=20, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = hist.model.predict(X_test_padded_seq)\n",
    "# pred_test = hist_test.model.predict(X_test_padded_seq)\n",
    "# preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "876/876 [==============================] - 1s 1ms/step\n",
      "1750/1750 [==============================] - 1s 629us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.7298478233759806, 0.6130136983579697],\n",
       " [0.3832950737816947, 0.8582857146263122])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_test = hist.model.evaluate(X_test_padded_seq, y_test)\n",
    "score_train = hist.model.evaluate(X_train_padded_seq, y_train)\n",
    "score_test, score_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed\n",
      "Compiled\n",
      "Fitted\n"
     ]
    }
   ],
   "source": [
    "# 2 Hidden layer (LSTM, Dense(20))\n",
    "input_layer = Input(shape=(X_train_padded_seq.shape[1],))\n",
    "e = Embedding(vocab_size,\n",
    "              100,\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=X_train_padded_seq.shape[1],\n",
    "              trainable=False)(input_layer)\n",
    "hidden1 = LSTM(32, return_sequences=False)(e)\n",
    "hidden2 = Dense(20, activation='relu')(hidden1)\n",
    "output_layer = Dense(1, activation='sigmoid')(hidden2)\n",
    "model2 = Model(inputs=input_layer, outputs=output_layer)\n",
    "print(\"Constructed\")\n",
    "model2.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "print(\"Compiled\")\n",
    "hist = model2.fit(X_train_padded_seq, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "print(\"Fitted\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "876/876 [==============================] - 1s 1ms/step\n",
      "1750/1750 [==============================] - 1s 721us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.9068888107391253, 0.5981735159817352],\n",
       " [0.30580400838170735, 0.8817142858505249])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_test = model2.evaluate(X_test_padded_seq, y_test)\n",
    "score_train = model2.evaluate(X_train_padded_seq, y_train)\n",
    "score_test, score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed\n",
      "Compiled\n",
      "Fitted\n",
      "876/876 [==============================] - 1s 1ms/step\n",
      "1750/1750 [==============================] - 1s 741us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.7707126241296394, 0.6438356164383562],\n",
       " [0.29843199603898185, 0.8880000001362391])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 Hidden layer (LSTM(32), Dense(10))\n",
    "input_layer = Input(shape=(X_train_padded_seq.shape[1],))\n",
    "e = Embedding(vocab_size,\n",
    "              100,\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=X_train_padded_seq.shape[1],\n",
    "              trainable=False)(input_layer)\n",
    "hidden1 = LSTM(32, return_sequences=False)(e)\n",
    "hidden2 = Dense(20, activation='relu')(hidden1)\n",
    "output_layer = Dense(1, activation='sigmoid')(hidden2)\n",
    "model3 = Model(inputs=input_layer, outputs=output_layer)\n",
    "print(\"Constructed\")\n",
    "model3.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "print(\"Compiled\")\n",
    "hist = model3.fit(X_train_padded_seq, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "print(\"Fitted\")\n",
    "\n",
    "score_test = model3.evaluate(X_test_padded_seq, y_test)\n",
    "score_train = model3.evaluate(X_train_padded_seq, y_train)\n",
    "score_test, score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed\n",
      "Compiled\n",
      "Fitted\n",
      "876/876 [==============================] - 1s 1ms/step\n",
      "1750/1750 [==============================] - 1s 724us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.699890801079197, 0.623287671505044],\n",
       " [0.4684965535572597, 0.8114285713604519])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 Hidden layer (LSTM(32), Dense(20), Dense(10))\n",
    "input_layer = Input(shape=(X_train_padded_seq.shape[1],))\n",
    "e = Embedding(vocab_size,\n",
    "              100,\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=X_train_padded_seq.shape[1],\n",
    "              trainable=False)(input_layer)\n",
    "hidden1 = LSTM(32, return_sequences=False)(e)\n",
    "hidden2 = Dense(20, activation='sigmoid')(hidden1)\n",
    "hidden3 = Dense(10, activation='relu')(hidden2)\n",
    "output_layer = Dense(1, activation='sigmoid')(hidden3)\n",
    "model4 = Model(inputs=input_layer, outputs=output_layer)\n",
    "print(\"Constructed\")\n",
    "model4.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "print(\"Compiled\")\n",
    "hist = model4.fit(X_train_padded_seq, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "print(\"Fitted\")\n",
    "\n",
    "score_test = model4.evaluate(X_test_padded_seq, y_test)\n",
    "score_train = model4.evaluate(X_train_padded_seq, y_train)\n",
    "score_test, score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ekremguzelyel/anaconda/lib/python3.6/site-packages/keras/activations.py:197: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed\n",
      "Compiled\n",
      "Fitted\n",
      "876/876 [==============================] - 1s 1ms/step\n",
      "1750/1750 [==============================] - 1s 706us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.8221887942046335, 0.5993150682209833],\n",
       " [0.45476821078572954, 0.7657142858505249])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 Hidden layer (LSTM(32), Dense(20, leakyrelu), Dense(10))\n",
    "input_layer = Input(shape=(X_train_padded_seq.shape[1],))\n",
    "e = Embedding(vocab_size,\n",
    "              100,\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=X_train_padded_seq.shape[1],\n",
    "              trainable=False)(input_layer)\n",
    "hidden1 = LSTM(32, return_sequences=False)(e)\n",
    "# LeakyReLU(10, )\n",
    "hidden2 = Dense(20, activation=LeakyReLU(alpha=5))(hidden1)\n",
    "hidden3 = Dense(10, activation='relu')(hidden2)\n",
    "output_layer = Dense(1, activation='sigmoid')(hidden3)\n",
    "model5 = Model(inputs=input_layer, outputs=output_layer)\n",
    "print(\"Constructed\")\n",
    "model5.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "print(\"Compiled\")\n",
    "hist = model5.fit(X_train_padded_seq, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "print(\"Fitted\")\n",
    "\n",
    "score_test = model5.evaluate(X_test_padded_seq, y_test)\n",
    "score_train = model5.evaluate(X_train_padded_seq, y_train)\n",
    "score_test, score_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHY THE SCORE GIVES TWO DIFFERENT RESULTS?**\n",
    "\n",
    "_ASK THIS QUESTION_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660/660 [==============================] - 0s 724us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7514940818150838, 0.6363636367248766]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(X_test_padded_seq, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5015174507732999,\n",
       " 0.5576631258579595,\n",
       " 0.6062215478901436,\n",
       " 0.6176024280115396,\n",
       " 0.6198786039905954,\n",
       " 0.6517450683757279,\n",
       " 0.6570561455848184,\n",
       " 0.6631259483162296,\n",
       " 0.6896813352661538,\n",
       " 0.6980273142027384,\n",
       " 0.7109256449165402,\n",
       " 0.7223065251283833,\n",
       " 0.7336874051593323,\n",
       " 0.7389984826397643,\n",
       " 0.7587253413359565,\n",
       " 0.7867981791496277,\n",
       " 0.7883156297420334,\n",
       " 0.8125948405772312,\n",
       " 0.8262518967229064,\n",
       " 0.8338391502276176]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= [i for i,j in enumerate(hist.history['acc'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x=[i for i,j in enumerate(hist.history['acc'])]\n",
    "plt.plot(x, hist.history['acc'])\n",
    "plt.plot(x, hist.history['loss'])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__IS THERE AN EASIER WAY TO CLASSIFY PREDICTIONS IN BINARY?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_test), len(preds))\n",
    "preds_binary =[]\n",
    "for i in range(len(preds)):\n",
    "    preds[i] = [1 if preds[i]>0.5 else 0][0]\n",
    "        \n",
    "# y_test ,preds\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW STUFF FOR NEW SEQUENCES\n",
    "\n",
    "__DIDN'T WORK__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import keras\n",
    "import labeled_functions\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import text_to_word_sequence, one_hot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_corpus, y = labeled_functions.load_with_path()\n",
    "X_train_corpus , X_test_corpus, y_train, y_test = train_test_split(X_corpus, y, test_size=1./3, random_state=42)\n",
    "len(X_train_corpus)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(X_train_corpus[400])\n",
    "X_train_sequence=[]\n",
    "length=0\n",
    "for i in X_train_corpus:\n",
    "    seq = text_to_word_sequence(text=i)\n",
    "    X_train_sequence.append(seq)\n",
    "    length = [len(set(seq)) if len(set(seq))>length else length][0]\n",
    "length"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "input_layer = Input(shape=(X_train_vector.shape[1],))\n",
    "embed_layer = Embedding(output_dim=64, input_dim=X_train_vector.shape[0]*X_train_vector.shape[1], input_length=X_train_vector.shape[1])(input_layer)\n",
    "hidden1 = LSTM(32, return_sequences=False)(e)\n",
    "# hidden2 = Dense(10, activation='relu')(hidden1)\n",
    "output_layer = Dense(1, activation='sigmoid')(hidden1)\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "# summarize layers\n",
    "print(model.summary())\n",
    "# plot graph\n",
    "# plot_model(model, to_file='recurrent_neural_network.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_vector, y_train, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TUTORIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n",
      "[[ 0  0  6  2]\n",
      " [ 0  0  3  1]\n",
      " [ 0  0  7  4]\n",
      " [ 0  0  8  1]\n",
      " [ 0  0  0  9]\n",
      " [ 0  0  0 10]\n",
      " [ 0  0  5  4]\n",
      " [ 0  0 11  3]\n",
      " [ 0  0  5  1]\n",
      " [12 13  2 14]]\n"
     ]
    }
   ],
   "source": [
    "# X_train_corpus\n",
    "docs = ['Well done!', 'Good work','Great effort',\n",
    "        'nice work', 'Excellent!',\n",
    "        'Weak','Poor effort!', 'not good', 'poor work',\n",
    "        'Could have done better.']\n",
    "# define class labels\n",
    "# y_train\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='pre')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 41, 5, 1, 411, 29, 4, 117, 40], [8, 208, 11, 182, 581, 19], [18, 13, 4, 19, 46], [18, 1266, 1267, 582, 1268, 5, 25, 36, 782, 9, 7, 23, 3, 783, 1269, 358, 209, 58, 29, 118, 412, 22, 1270, 413, 9, 37, 6, 1, 477, 159, 14, 9, 7, 237]]\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    3   41    5    1  411   29    4  117   40]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    8  208   11  182  581   19]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0   18   13    4   19   46]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0   18 1266 1267  582 1268    5   25   36\n",
      "   782    9    7   23    3  783 1269  358  209   58   29  118  412   22\n",
      "  1270  413    9   37    6    1  477  159   14    9    7  237]]\n"
     ]
    }
   ],
   "source": [
    "docs = X_corpus\n",
    "labels = y\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs[:4])\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 124\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='pre')\n",
    "print(padded_docs[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('../../../glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=124, trainable=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
